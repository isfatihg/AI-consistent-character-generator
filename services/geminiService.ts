
import { GoogleGenAI, Modality } from "@google/genai";

const fileToGenerativePart = async (file: File) => {
  const base64EncodedDataPromise = new Promise<string>((resolve) => {
    const reader = new FileReader();
    reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
    reader.readAsDataURL(file);
  });
  return {
    inlineData: { data: await base64EncodedDataPromise, mimeType: file.type },
  };
};

const urlToGenerativePart = async (url: string) => {
    const response = await fetch(url);
    const blob = await response.blob();
    const base64EncodedDataPromise = new Promise<string>((resolve) => {
        const reader = new FileReader();
        reader.onloadend = () => resolve((reader.result as string).split(',')[1]);
        reader.readAsDataURL(blob);
    });
    return {
        inlineData: { data: await base64EncodedDataPromise, mimeType: blob.type },
    };
};

export const generateCharacterFromText = async (
  userPrompt: string
): Promise<string> => {
  if (!process.env.API_KEY) {
    throw new Error("API_KEY environment variable not set");
  }
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  const finalPrompt = `Generate a full-body character portrait based on the following description: "${userPrompt}". The character should be centered against a simple, neutral background.`;

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash-image',
    contents: {
      parts: [
        { text: finalPrompt },
      ],
    },
    config: {
      responseModalities: [Modality.IMAGE],
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (part.inlineData) {
      return `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`;
    }
  }

  throw new Error("No image was generated by the API.");
};


export const editImage = async (
  sourceImage: File | string,
  userPrompt: string
): Promise<string> => {
  if (!process.env.API_KEY) {
    throw new Error("API_KEY environment variable not set");
  }
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  const imagePart = typeof sourceImage === 'string'
    ? await urlToGenerativePart(sourceImage)
    : await fileToGenerativePart(sourceImage as File);

  const finalPrompt = `Your task is to edit an image based on user instructions.
1. Analyze the provided image.
2. Apply the following user instruction: "${userPrompt}"
3. The final image should be a modified version of the original image, reflecting the user's request. Ensure the core subject and style of the original image are preserved unless the instruction explicitly asks to change them.`;

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash-image',
    contents: {
      parts: [
        imagePart,
        { text: finalPrompt },
      ],
    },
    config: {
      responseModalities: [Modality.IMAGE],
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (part.inlineData) {
      return `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`;
    }
  }

  throw new Error("No image was generated by the API.");
};


export const generateImageFromPose = async (
  characterImage: File | string,
  poseImageUrl: string,
  userPrompt: string
): Promise<string> => {
  if (!process.env.API_KEY) {
    throw new Error("API_KEY environment variable not set");
  }
  const ai = new GoogleGenAI({ apiKey: process.env.API_KEY });

  const characterPart = typeof characterImage === 'string' 
    ? await urlToGenerativePart(characterImage)
    : await fileToGenerativePart(characterImage as File);
    
  const posePart = await urlToGenerativePart(poseImageUrl);

  const finalPrompt = `Your task is to generate a consistent character.
1. Analyze the character in the first image. Pay close attention to all details: clothing, hairstyle, colors, facial features, and overall style.
2. Analyze the pose in the second image. This is the target pose for the character.
3. Redraw the character from the first image in the exact pose shown in the second image.
4. The final image should ONLY contain the character in the new pose. Maintain the background from the original character image unless specified otherwise by the user instructions. Do not include the pose reference image in the output.
5. If user instructions are provided, apply them to the newly generated image. Instructions: "${userPrompt || 'No additional instructions.'}"`;

  const response = await ai.models.generateContent({
    model: 'gemini-2.5-flash-image',
    contents: {
      parts: [
        characterPart,
        posePart,
        { text: finalPrompt },
      ],
    },
    config: {
      responseModalities: [Modality.IMAGE],
    },
  });

  for (const part of response.candidates[0].content.parts) {
    if (part.inlineData) {
      return `data:${part.inlineData.mimeType};base64,${part.inlineData.data}`;
    }
  }

  throw new Error("No image was generated by the API.");
};